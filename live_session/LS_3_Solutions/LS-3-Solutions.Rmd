---
title: "Unit 3 Live Session"
output: 'pdf_document'  
classoption: landscape
---

# Discrete Response Model Part 3

![South Hall](./images/south_hall.png){width=50%}


\newpage

## Class Announcements

- HW 3 is this week

- Lab-1 due in 3 weeks

## Roadmap

**Rearview Mirror**

- Discuss why the classical linear regression model is not the best choice for the binary response model

- Discuss logistic regression models, the most important special case of generalized linear models (GLMs). 

**Today**

- Variable transformation: interactions among explanatory variables and quadratic terms

- Categorical explanatory variables

- Convergence criteria and complete separation


**Looking Ahead**

- Multinomial probability distribution, 

- $IJ$ contingency tables and inference using contingency tables

- Nominal response models

- Ordinal logistic regression model

\newpage

## Start-up Code

```{r message=FALSE, warning=FALSE}
# Insert the function to *tidy up* the code when they are printed out
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)


# Start with a clean R environment
rm(list = ls())

# Load libraries
## Load a set of packages inclusing: broom, cli, crayon, dbplyr , dplyr, dtplyr, forcats,
## googledrive, googlesheets4, ggplot2, haven, hms, httr, jsonlite, lubridate , magrittr, 
## modelr, pillar, purrr, readr, readxl, reprex, rlang, rstudioapi, rvest, stringr, tibble, 
## tidyr, xml2
library(tidyverse)

## to load glow500 from "Applied Logistic Regression" by D.W. Hosmer, S. Lemeshow, and R.X. Sturdivant (3rd ed., 2013) 
library(aplore3)

## provides  many functions useful for data analysis, high-level graphics, and utility operations like describe()
library(Hmisc)

## to work with "grid" graphics
library(gridExtra)

## To generate regression results, tables, and plots
library(finalfit)

## To produces LaTeX code, HTML/CSS code and ASCII text for well-formatted tables
library(stargazer)
```

\newpage

## Discusion: Complete Separation 

- What is complete separation in logistic regression?

**A complete separation or sometimes also referred to as perfect prediction in logistic regression, happens when the response variable is completely separated by an explanatory variable or a linear combination of them.**

- Is there any problem with the following data set?

```{r}
df_0 <- data.frame(y =  c(0,0,0,0,1,1,1,1), x1 = c(1,2,3,3,5,6,10,11), x2 = c(3,2,-1,-1,2,4,1,0))
plot(df_0$x1, df_0$y)
```

**Here, observations with $Y = 0$ all have values of $X1 \le 3$ and observations with $Y = 1$ all have values of $X1>3$. In other words X1 predicts Y perfectly, and we have complete separation**

- What happens when we try to fit a logistic regression model of Y on X1 and X2?

```{r}
mod.logit.complete<- glm(y~ x1+x2, family=binomial(link = logit), data = df_0)
summary(mod.logit.complete)

```

**glm() report a Warning message: "glm.fit: fitted probabilities numerically 0 or 1 occurred", which signals perfect separation as we can predict cases perfectly. Also, estimated coefficients have extremely large estimated standard deviations, and we can not interpret them reliably. This is due to the likelihood function not having a well defined optimum (we can increase the likelihood and therefore fit by increasing the coefficient on X1 to infinity).**

**If we're only interested in classifying response levels of y, complete separation is not necessarily a bad thing. But The problem is that the estimated model by maximum likelihood does not have a good interpretation or stable estimate.**

- What are the techniques to deal with complete separation? 

**There are a few possible options: (1) collect more data to potentially find examples that remove complete separation, (2) modify the likelihood function by adding a penalized term akin to lasso regression for the binary case, or (3) drop the variables causing complete separation. Since finding which exact variables are causing the complete separation, the better solution is (2) and adding a penalized term to shrink the coefficients as in lasso or ridge regression.**

\newpage

## Case Study: Osteoporosis in Women

### Introduction

In osteoporosis, bones become weak and brittle, so weak that even bending over or coughing can fracture them. Hip, wrist, and spine fractures are the most common osteoporosis-related fractures.

All races of people are at risk for osteoporosis. However, white and Asian women, particularly those that are post menopause, are at the greatest risk. A healthy diet, weight-bearing exercises, and medications can strengthen weak bones or prevent their loss. (Mayo Clinic)

Here, Our goal is description of the data:

  - **How factors such as age and weight are related to the fracture rates among older women?**
  
### Data Description

This sample comes from the Global Longitudinal Study of Osteoporosis in Women (GLOW).

The data set includes information on 500 subjects enrolled in this study.

Install and load the aplore3 library to use the glow500 dataset and understand the structure dataset.

We summarize some of the variables that we will use:
  
  - PRIORFRAC: History of prior fracture
  - AGE: Age at enrollment
  - WEIGHT: Weight at enrollment (Kilograms)
  - HEIGHT: Height at enrollment (Centimeters)
  - BMI: Body mass index ($kg/m^2$)
  - PREMENO: Menopause before age 45
  - FRACTURE: Any fracture in first year of follow up
  - RATERISK: Self-reported risk of fracture
  - SMOKE: Former or current smoker

\newpage

### Descriptive Statistics

- First, load and check the data set. 

```{r}
df = glow500 %>%
  dplyr::select(fracture, age, priorfrac, premeno, raterisk, smoke, bmi)


head(df) %>%
  knitr::kable()
#str(df)
#glimpse(df)
#summary(df)
#describe(df)
```

\newpage

#### Univariate Analysis

- The response (or dependent) variable of interest, fracture in the first year of follow-up as FRACTURE, is a binary variable taking the type “factor”. 

- Use the following code to review the distribution of the response variable (FRACTURE). What do you discover?

```{r}

df %>%
  count(fracture) %>%
  mutate(prop = round(prop.table(n),2)) %>%
  kable(col.names = c('Fracture', 'N', "Proportion"))


df %>%
  ggplot(aes(x= fracture, y = ..prop.., group = 1)) + 
  geom_bar(fill = 'DarkBlue', color = 'black') +
  geom_text(stat='count', aes(label=..count..), vjust=-1) + 
  xlab("Fracture") +
  ylab("Proportion") +
  ylim(0,1)


```


**From the bar plot and the table, 25% of subjects in our sample suffered a fracture. If we didn't have any other information, 25%  would be MLE of the probability of having a fracture in older women.**

For metric variables, histograms allow us to determine the shape of the distribution and look for outliers. 

- Use a density plot to examine the distribution of age and BMI. What do you learn?

```{r}
p1 <- df %>% 
  ggplot(aes(x = age)) +
  geom_density(aes(y = ..density..,color = fracture, fill = fracture),alpha=0.2) +
  ggtitle("Distribution of Subjects' Age") + 
  theme(plot.title = element_text(lineheight=1, face="bold")) +
  xlab("Yaer") +
  ylab("Density")


p2 <-df %>% 
  ggplot(aes(x = bmi)) +
  geom_density(aes(y = ..density.., color = fracture, fill = fracture),alpha=0.2) +
  ggtitle("Distribution of Subjects'BMI") + 
  theme(plot.title = element_text(lineheight=1, face="bold")) +
  xlab("Body mass index") +
  ylab("Density")

grid.arrange(p1, p2, nrow = 1, ncol = 2)

```

**Age has a higher age in women with fractures than women without fractures. BMI distributions have  almost the same mean and same variance in both groups with and without fracture, so probably BMI is not a useful variable to classify these two groups **

\newpage

#### Bivariate Analysis

- Use boxplots to examine how the fracture is correlated with age and BMI.
  
   - The coord_flip() function keeps the dependent variable on the y-axis.

```{r}
p3 <- df %>%
 ggplot(aes(fracture, bmi)) +
  geom_boxplot(aes(fill = fracture)) + 
  coord_flip() +
  ggtitle("Subjects' BMI by Fracture in the First Year") + 
  theme(plot.title = element_text(lineheight=1, face="bold")) +
  ylab("Body mass index") +
  xlab("Fracture") 

p4 <- df %>%
 ggplot(aes(fracture, age)) +
  geom_boxplot(aes(fill = fracture)) + 
  coord_flip() +
  ggtitle(" Age by Fracture in the First Year") + 
  theme(plot.title = element_text(lineheight=1, face="bold")) +
  ylab("Year") +
  xlab("Fracture") 

grid.arrange(p3, p4, nrow = 2, ncol = 1)
```

```{r}
p5 <- df %>%
  ggplot(aes(x=priorfrac, 
             y = ..prop.., 
             group = fracture,
             fill = fracture)) + 
  geom_bar( position = 'dodge') +
  geom_text(stat='count', 
            aes(label=..count..), 
            vjust=-1, 
            position = position_dodge(width = 1)) +
  xlab("prior fracture") +
  ylab("Proportion") +
  ylim(0,1) +
  labs(fill = "fracture")


p6 <- df %>%
  ggplot(aes(x=raterisk, 
             y = ..prop.., 
             group = fracture,
             fill = fracture)) + 
  geom_bar( position = 'dodge') +
  geom_text(stat='count', 
            aes(label=..count..), 
            vjust=-1, 
            position = position_dodge(width = 1)) +
  xlab("Self-reported risk of fracture") +
  ylab("Proportion") +
  ylim(0,1) +
  labs(fill = "fracture")


p7 <- df %>%
  ggplot(aes(x= premeno, 
             y = ..prop.., 
             group = fracture,
             fill = fracture)) + 
  geom_bar( position = 'dodge') +
  geom_text(stat='count', 
            aes(label=..count..), 
            vjust=-1, 
            position = position_dodge(width = 1)) +
  xlab("Menopause before age 45") +
  ylab("Proportion") +
  ylim(0,1) +
  labs(fill = "fracture")

p8 <- df %>%
  ggplot(aes(x= smoke, 
             y = ..prop.., 
             group = fracture,
             fill = fracture)) + 
  geom_bar( position = 'dodge') +
  geom_text(stat='count', 
            aes(label=..count..), 
            vjust=-1, 
            position = position_dodge(width = 1)) +
  xlab("Former or current smoker") +
  ylab("Proportion") +
  ylim(0,1) +
  labs(fill = "fracture")

grid.arrange(p5, p6, p7, p8, nrow = 2, ncol = 2)
```

**From these box plots, we can see the women who suffered from a fracture are older, but both groups have the same distribution of BMI.**

**From the plots above, we see that the women with a history of prior fracture, and a high self-reported risk of fracture, have a higher probability of having a fracture in the first year of study. But, smokers and no smokers and women with or without menopause before 45 have the same probability of having a fracture. so smokers and menopause do not help classify these two groups, and we're not going to use them for modeling**

\newpage

- Use the convenient summary_factorlist() function from the finalfit package to tabulate data. What do you learn from the EDA?

```{r}
dependent <- "fracture"
explanatory <- c("bmi","age", "priorfrac", "premeno", "raterisk", "smoke")
df %>% 
  summary_factorlist(dependent, explanatory, add_dependent_label = TRUE) %>%
  knitr::kable()
```
 
\newpage

### Model Development

#### Simple Binary Logistic Regression

- Estimate the following base model and interpret the results.

$$ logit(\pi_i) =\beta_0 + \beta_1 bmi + \beta_2 age + u$$

```{r}
mod.logit.1 <- glm(fracture ~  bmi + age, family = binomial(link = logit), data = df)

summary(mod.logit.1)
```

**As we expected from EDA, only the age coefficient is statistically significant. Age is positively correlated with the probability of having a fracture, and holding BMI constant, For one year increase in age, the log odds of having a fracture increases by 0.05 or 5%.**

- Recall:

$$
OR = \frac{Odds_{x_k+c}}{Odds_{x_k}}=exp(c \beta_k)
$$

- Find and interpret the estimated odds ratios for a 10-unit increase in age. 


```{r}
round(cbind(exp(10*coef(mod.logit.1)[3])),2)
```

**The estimated odds of having a fracture change by 1.77 times for every 10-year increase in age, or it's 77% higher**

\newpage

#### Categorical explantory variables

- First, check the levels attribute of priorfrac and raterisk

- Estimate the following model with three categorical variables and interpret the results.

$$logit(\pi_i) =\beta_0 + \beta_1 bmi + \beta_2 age + \beta_3 priorfrac+ \beta_4 rateriskSame + \beta_5 rateriskGreater+ u$$

```{r}
levels(df$priorfrac)
levels(df$raterisk)

#set reference levels in factors to make interpretation easier
df$priorfrac<-relevel(df$priorfrac, ref="No")
df$raterisk<-relevel(df$raterisk, ref="Less")

mod.logit.2 <- glm(fracture ~  bmi + age + priorfrac + raterisk, family = binomial(link = logit), data = df)

summary(mod.logit.2)
```

**Here, six parallel lines are being estimated, one for each combination of priorfrac and raterisk, and each line has a different intercept but the same slope for age and BMI.**

**1- priorfrac = 0 and raterisk = Less**
$$logit(\pi_i) =\beta_0 + \beta_1 bmi + \beta_2 age$$

**$\beta_0 = -6.17617$ is log-odds in women with no prior fracture and less risk when age and BMI are zero**

**2- priorfrac = 0 and raterisk = Same**
$$logit(\pi_i) = (\beta_0 + \beta_4) + \beta_1 bmi + \beta_2 age$$

**$\beta_0+\beta_4$ is log-odds in women with no prior fracture and the same risk when age and BMI are zero**

**3- priorfrac = 0 and raterisk = Greater**
$$logit(\pi_i) = (\beta_0 + \beta_5) +  \beta_1 bmi + \beta_2 age$$
**$\beta_0+\beta_5$ is log-odds in women with no prior fracture and greater risk, when age and BMI are zero**

**4- priorfrac = 1 and raterisk =less**
$$logit(\pi_i) =(\beta_0 + \beta_3) + \beta_1 bmi + \beta_2 age $$
**$\beta_0+\beta_3$ is log-odds in women with  prior fracture and less risk, when age and BMI are zero**

**5- priorfrac = 1 and raterisk = Same**
$$logit(\pi_i) =(\beta_0 + \beta_3 + \beta_4) +  \beta_1 bmi + \beta_2 age$$
**$\beta_0+\beta_3 + \beta_4$ is log-odds in women with  prior fracture and same risk, when age and BMI are zero**

**6- priorfrac = 1  and raterisk = Greater**
$$logit(\pi_i) =(\beta_0 + \beta_3 + \beta_5) +  \beta_1 bmi + \beta_2 age$$
**$\beta_0+\beta_3 + \beta_5$ is log-odds in women with  prior fracture and greater risk, when age and BMI are zero**

**In all six models, for a one year increase in age, the log odds of having a fracture increase by 0.05 or 5%**

- Recall that for categorical explanatory variable:
  
  - Odds ratio comparing k level to reference level is:

$$
OR = \frac{Odds_{x_k}}{Odds_{x_0}}=exp( \beta_k)
$$


 - and odds ratio comparing k level to another level like k-1 is:
  
$$
OR = \frac{Odds_{x_k}}{Odds_{x_{k-1}}}=exp( \beta_k - \beta_{k-1})
$$

- Find and interpret the estimated all odds ratios for prior risk and raterisk variable.

```{r}
#since all except for last odds ratio compare to reference, we can estimate them using exponentiated coefficients from the glm output directly
round(exp(coef(mod.logit.2)),2)

oods_rateriskGreater_Same <- round(exp(coef(mod.logit.2)[6] - coef(mod.logit.2)[5]),2)
oods_rateriskGreater_Same
```

**The estimated odds of fracture are 94% higher in women with prior fracture v.s. women without prior fracture, where other variables are held constant**
    
    
**The estimated odds of having a fracture is 72% higher in women with the same risk v.s. women with less risk hold other variables constant.**
  
    
**The estimated odds of fracture change by 2.5 times for women with greater risk level v.s. less risk level, hold other variables constant**   
    
    
**The estimated odds of having a fracture is 46% higher in women with greater risk vs. women with the same risk, holding other variables constant.**
    

\newpage

#### Interaction Terms

-  What is the purpose of an interaction term? 

- Estimate the following model with interaction terms between age and categorical variables and interpret the results.

\begin{align*}
logit(\pi_i) &=\beta_0 + \beta_1 bmi + \beta_2 age + \beta_3 priorfrac+\\ 
& \beta_4 rateriskSame + \beta_5 rateriskGreater + \beta_6 age*priorfrac+ \\
& \beta_7 age \cdot rateriskSame +  \beta_8 age\cdot rateriskGreater+ u
\end{align*}


```{r}
mod.logit.3 <- glm(fracture ~ age + bmi+ priorfrac + raterisk + age:priorfrac + age:raterisk , 
                   family = binomial(link = logit), data = df)


summary(mod.logit.3)
```

**Here, we estimated six lines with six different intercepts and six different slopes of age, one for each combination of priorfrac and raterisk.**

**1- priorfrac = 0 and raterisk = Less**
$$logit(\pi_i) =\beta_0 + \beta_1 bmi + \beta_2 age$$

**$\beta_2$ is an increase in log-odds for a year increase in age in women with no prior fracture and less risk**

**2- priorfrac = 0 and raterisk = Same**
$$logit(\pi_i) = (\beta_0 + \beta_4) + \beta_1 bmi + (\beta_2 + \beta7) age$$

**$\beta_2+\beta_7$  is an increase in log-odds for a year increase in age in women with no prior fracture and the same risk**

**3- priorfrac = 0 and raterisk = Greater**
$$logit(\pi_i) = (\beta_0 + \beta_5) +  \beta_1 bmi + (\beta_2 + \beta_8) age$$
**$\beta_2+\beta_8$  is an increase in log-odds for a year increase in age in women with no prior fracture and greater risk**

**4- priorfrac = 1 and raterisk =less**
$$logit(\pi_i) =(\beta_0 + \beta_3) + \beta_1 bmi + (\beta_2+\beta_6)age $$
**$\beta_2+\beta_6$  is an increase in log-odds for a year increase in age in women with  prior fracture and less risk**

**5- priorfrac = 1 and raterisk = Same**
$$logit(\pi_i) =(\beta_0 + \beta_3 + \beta_4) +  \beta_1 bmi + (\beta_2+\beta_6+\beta_7)age$$
**$\beta_2+\beta_6 + \beta_7$  is an increase in log-odds for a year increase in age in women with  prior fracture and same risk**

**6- priorfrac = 1  and raterisk = Greater**
$$logit(\pi_i) =(\beta_0 + \beta_3 + \beta_5) +  \beta_1 bmi + (\beta_2+\beta_6+\beta_8)age$$

**$\beta_2+\beta_6 + \beta_8$  is an increase in log-odds for a year increase in age in women with  prior fracture and Greater risk**


**In all six models, BMI has the same effects on log-odds**

- Recall that for the following model with interaction term :

$$
y = \beta_0 + \beta_1 * x_1 + ......+ \beta_k * x_k + \beta_{k+1} * x_1*x_{k} + u 
$$

$$
OR = \frac{Odds_{x_k + c}}{Odds_{x_k}}= exp(c*( \beta_k + \beta_{k+1} *x_1))
$$

- Find and interpret the odds ratio of a 10-year increase in age for people with and without prior fracture. 

```{r}
beta.hat <- mod.logit.3$coefficients
#beta.hat

c <- 10
prior_fracture <- c(0,1)
log.OR.age <- c*(beta.hat[2]+beta.hat[7]*prior_fracture)
OR.age <- exp(log.OR.age)
round(data.frame(prior_fracture  = prior_fracture , OR.hat = OR.age),2)
```

**The odds of having a fracture change by 2.58 times for a 10-year increase in age in women without prior fracture and by 1.52 times in women with the previous fracture. The odds ratio of 10 years increase in age is smaller in women with the previous fracture.**


\newpage

### Statistical Inference

#### Hypothesis Test

- Perform the likelihood ratio test comparing two models with and without BMI and age:raterisk.

  - $H_0: \beta_{bmi} = \beta_{age:raterisk}= 0$

  - $H_a: \beta_{bmi} or \beta_{age:raterisk} \ne 0$


```{r}
mod.logit.4 <- glm(fracture ~ age + priorfrac + raterisk + age:priorfrac, family = binomial(link = logit), data = df)
summary(mod.logit.4)

anova(mod.logit.4, mod.logit.3, test = "Chisq")
```


**As the p-value is large, exceeding 0.05, we fail to reject the null hypothesis that BMI and interaction terms between age and risk risk are different from zero. Because BMI and this interaction term are both individually and together insignificant, we remove them from the model**

\newpage

####  Confidence Interval

- Recall when:

$$
OR = \frac{Odds_{x_k + c}}{Odds_{x_k}}= exp(c*( \beta_k + \beta_{k+1} *x_1))
$$

- Then $(1- \alpha)$ wald confidence interval is:

$$
exp \left(c* ( \widehat{\beta_k} + \widehat{\beta_{k+1}} * x_1) \pm Z_{1-\alpha/2} \sqrt{\widehat{Var}(c* (\widehat{\beta}_k + \widehat{\beta_{k+1}}*x_1))} \right)
$$

- with

$$
\widehat{Var}(c*(\hat{\beta}_k +  \hat{\beta}_{k+1} x_1)) = c^2\widehat{Var}(\hat{\beta_k}) + c^2*x_1^2* \widehat{Var}(\widehat{\beta_{k+1}}) +c^2*2*x_1*\widehat{Cov}(\hat{\beta}_k,\hat{\beta}_{k+1})
$$

- Use model.logit.4 and compute the odds ratio and wald confidence interval of prior fracture for 55, 65, 75, 85 years old women.

```{r}
beta.hat <- mod.logit.4$coefficients
#c is 1 since is either prior fraction = Yes or prior fracture = No
c <- 1
#fixed age levels we are interested in i.e. x1
age <- seq(from = 55, to = 85, by = 10)

log.OR.prior_fracture <- c * (beta.hat[3] + beta.hat[6] * age)
OR.prior_fracture <- exp(log.OR.prior_fracture)

cov.mat <- vcov(mod.logit.4)
var.log.OR <- c^2 * (cov.mat[3,3] + age^2 * cov.mat[6,6] + 2 * age * cov.mat[3,6])

ci.log.OR.low <- exp(log.OR.prior_fracture - qnorm(p = 0.975,)*sqrt(var.log.OR))
ci.log.OR.up <- exp(log.OR.prior_fracture + qnorm(p = 0.975,)*sqrt(var.log.OR))

round(data.frame(age = age, OR.hat =  OR.prior_fracture, OR.low = ci.log.OR.low, OR.up = ci.log.OR.up),2)
```

**With 95% confidence,  the odds of having a fracture change by an amount between 2.14 to 14.27 times in women with prior fractures verse women without previous fractures for 55 years-old women. The odd ratio of previous fracture and their CI decrease as women get older, which indicate that prior fracture has a smaller effect on a fracture as women get older. **


\newpage

### Final Visualization

Plot the estimated logistic regression model with and without age and prior fracture interaction for women with greater self-reported risk. Are there any interesting differences between the logistic regression model with and without the interaction term?

```{r}
par(mfrow = c(1,2))

## models
mod.logit.without <- glm(fracture ~ age + priorfrac + raterisk , family = binomial(link = logit), data = df)
mod.logit.with <- glm(fracture ~ age + priorfrac + raterisk + age:priorfrac, family = binomial(link = logit), data = df)


#### Without interaction term
curve(expr = predict(object = mod.logit.without,
                     newdata = data.frame(age = x, priorfrac = "No", raterisk= "Greater" ), 
                     type = "response"), col = "red", lty = "solid", xlim = c(50,100),
      ylim = c(0,1), ylab = "Estimated probability", main = "Without Interaction",
      xlab = "Age", panel.first = grid(col = "gray", lty = "dotted"), cex.main = 0.9, lwd = 1)

curve(expr = predict(object = mod.logit.without,
                     newdata = data.frame(age = x, priorfrac = "Yes", raterisk= "Greater" ),
                     type = "response"), col = "blue", lty = "dotdash", lwd = 1, add = TRUE)

legend(x = 50, y = 0.9, legend = c("Prior fracture = 0", "Prior fracture  = 1"), 
       lty = c("solid", "dotdash"), col = c("red", "blue"),
       lwd = c(1,1), bty = "n")


##### with interaction term
curve(expr = predict(object = mod.logit.with,
                     newdata = data.frame(age = x, priorfrac = "No", raterisk= "Greater" ), 
                     type = "response"), col = "red", lty = "solid", xlim = c(50,100),
      ylim = c(0,1), ylab = "Estimated probability", main = "With Interaction",
      xlab = "Age", panel.first = grid(col = "gray", lty = "dotted"), cex.main = 0.9, lwd = 1)

curve(expr = predict(object = mod.logit.with,
                     newdata = data.frame(age = x, priorfrac = "Yes", raterisk= "Greater" ), type = "response"),
      col = "blue", lty = "dotdash", lwd = 1, add = TRUE)

legend(x = 50, y = 0.9, legend = c("Prior fracture  = 0", "Prior fracture  = 1"),
       lty = c("solid", "dotdash"), col = c("red", "blue"),
       lwd = c(1,1), bty = "n")


```

**In the left plot without interaction term, the estimated probability of having a fracture is always greater for women with prior fractures. But, in the right plot, we can see that fracture probability is higher for women with a previous fracture below the age of 84 or 85. But the red curve has a higher slope, the probability of fracture increases faster in women without the prior fracture, and as a result, they have a higher probability of fracture after age 84 or 85.**

\newpage

### Final Report

- Display all estimated logistic models in a regression table. How robust are your results?

```{r}

# uncomment and run the code
# 
stargazer(mod.logit.1, mod.logit.2, mod.logit.3, mod.logit.4,type = "text", omit.stat = "f",
                    star.cutoffs = c(0.05, 0.01, 0.001), title = "Table 1: 
           The estimated relationship between risk of fracture and risk factors")


```
**In all models, coefficients of age and priorfracYes are statistically significant with positive coefficients, which is a sign of the robustness of these effects.**

\newpage

## Reminders

1. Before the next live session: 
    1. Complete the homework that builds on this unit (HW-3)
    2. Complete all videos and reading for unit 4
